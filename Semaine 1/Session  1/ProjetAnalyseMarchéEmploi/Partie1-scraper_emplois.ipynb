{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse du March√© des Emplois Tech\n",
        "## Projet  - Session 1\n",
        "\n",
        "Ce notebook vous guide √† travers le processus de scraping et d'analyse du march√© de l'emploi tech.\n",
        "\n",
        "**Objectifs :**\n",
        "- Scraper des donn√©es d'emplois tech depuis des sites sp√©cialis√©s\n",
        "- Extraire des informations cl√©s : titre, entreprise, localisation, type de contrat, niveau d'exp√©rience\n",
        "- Analyser les technologies recherch√©es et les tendances du march√©\n",
        "- Sauvegarder les donn√©es pour visualisation\n",
        "\n",
        "**Focus de ce projet :**\n",
        "- Analyse des types de contrats (Remote/Hybrid/On-site)\n",
        "- Niveau d'exp√©rience requis\n",
        "- Stack technique recherch√©e\n",
        "- Tendances g√©ographiques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports et Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "REQUEST_DELAY = 2  # D√©lai en secondes entre les requ√™tes\n",
        "MAX_JOBS_TO_SCRAPE = 50  # Limite pour la d√©monstration\n",
        "\n",
        "# Headers pour simuler un navigateur\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recup√©rer la liste de l'URL des emploies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fonctions Utilitaires pour l'Extraction\n",
        "\n",
        "Ces fonctions extraient des informations sp√©cifiques depuis les descriptions d'emploi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_work_mode(text):\n",
        "    \"\"\"\n",
        "    D√©tecte le type de contrat (Remote/Hybrid/On-site) depuis le texte\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"Non sp√©cifi√©\"\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    # Mots-cl√©s pour remote\n",
        "    remote_keywords = ['remote', 't√©l√©travail', 'work from home', 'wfh', 'fully remote', '100% remote']\n",
        "    # Mots-cl√©s pour hybrid\n",
        "    hybrid_keywords = ['hybrid', 'hybride', 'partially remote', 'flexible', '2-3 days']\n",
        "    # Mots-cl√©s pour on-site\n",
        "    onsite_keywords = ['on-site', 'on site', 'onsite', 'office', 'bureau', 'pr√©sentiel']\n",
        "    \n",
        "    remote_count = sum(1 for keyword in remote_keywords if keyword in text_lower)\n",
        "    hybrid_count = sum(1 for keyword in hybrid_keywords if keyword in text_lower)\n",
        "    onsite_count = sum(1 for keyword in onsite_keywords if keyword in text_lower)\n",
        "    \n",
        "    if remote_count > 0 and remote_count >= hybrid_count:\n",
        "        return \"Remote\"\n",
        "    elif hybrid_count > 0:\n",
        "        return \"Hybrid\"\n",
        "    elif onsite_count > 0:\n",
        "        return \"On-site\"\n",
        "    else:\n",
        "        return \"Non sp√©cifi√©\"\n",
        "\n",
        "def extract_experience_level(text):\n",
        "    \"\"\"\n",
        "    Extrait le niveau d'exp√©rience requis depuis le texte\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"Non sp√©cifi√©\"\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    # Patterns pour diff√©rents niveaux\n",
        "    patterns = {\n",
        "        \"Junior\": [r'junior', r'entry level', r'0-2 years', r'1-2 years', r'debutant'],\n",
        "        \"Mid-level\": [r'mid-level', r'mid level', r'2-5 years', r'3-5 years', r'intermediate'],\n",
        "        \"Senior\": [r'senior', r'5\\+ years', r'5+ years', r'experienced', r'exp√©riment√©'],\n",
        "        \"Lead/Principal\": [r'lead', r'principal', r'staff', r'architect', r'10\\+ years']\n",
        "    }\n",
        "    \n",
        "    for level, pattern_list in patterns.items():\n",
        "        for pattern in pattern_list:\n",
        "            if re.search(pattern, text_lower):\n",
        "                return level\n",
        "    \n",
        "    return \"Non sp√©cifi√©\"\n",
        "\n",
        "def extract_tech_stack(text):\n",
        "    \"\"\"\n",
        "    Extrait les technologies mentionn√©es dans la description\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    \n",
        "    # Liste de technologies courantes\n",
        "    tech_keywords = {\n",
        "        'Python', 'JavaScript', 'Java', 'TypeScript', 'Go', 'Rust', 'C++', 'C#',\n",
        "        'React', 'Vue', 'Angular', 'Node.js', 'Django', 'Flask', 'FastAPI',\n",
        "        'AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes', 'Terraform',\n",
        "        'PostgreSQL', 'MongoDB', 'MySQL', 'Redis', 'Elasticsearch',\n",
        "        'Git', 'CI/CD', 'Jenkins', 'GitHub Actions',\n",
        "        'Machine Learning', 'TensorFlow', 'PyTorch', 'Scikit-learn',\n",
        "        'GraphQL', 'REST API', 'Microservices', 'Kafka', 'RabbitMQ'\n",
        "    }\n",
        "    \n",
        "    found_techs = []\n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    for tech in tech_keywords:\n",
        "        # Recherche insensible √† la casse avec word boundaries\n",
        "        pattern = r'\\b' + re.escape(tech.lower()) + r'\\b'\n",
        "        if re.search(pattern, text_lower, re.IGNORECASE):\n",
        "            found_techs.append(tech)\n",
        "    \n",
        "    return found_techs\n",
        "\n",
        "def extract_salary_range(text):\n",
        "    \"\"\"\n",
        "    Extrait la fourchette salariale depuis le texte\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None, None\n",
        "    \n",
        "    # Patterns pour trouver les salaires (ex: $80k-$120k, ‚Ç¨50,000-‚Ç¨70,000)\n",
        "    patterns = [\n",
        "        r'\\$?(\\d+)[kK]?\\s*-\\s*\\$?(\\d+)[kK]?',  # $80k-$120k ou 80k-120k\n",
        "        r'‚Ç¨?(\\d+)[,.]?\\d*\\s*-\\s*‚Ç¨?(\\d+)[,.]?\\d*',  # ‚Ç¨50,000-‚Ç¨70,000\n",
        "        r'(\\d+)\\s*to\\s*(\\d+)\\s*k',  # 80 to 120k\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            try:\n",
        "                min_sal = int(match.group(1).replace(',', '').replace('.', ''))\n",
        "                max_sal = int(match.group(2).replace(',', '').replace('.', ''))\n",
        "                # Convertir en milliers si n√©cessaire\n",
        "                if min_sal < 1000:\n",
        "                    min_sal *= 1000\n",
        "                if max_sal < 1000:\n",
        "                    max_sal *= 1000\n",
        "                return min_sal, max_sal\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    return None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fonction Principale d'Extraction\n",
        "\n",
        "Cette fonction extrait toutes les donn√©es d'une page d'emploi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_job_details_from_aijobs(url, soup=None):\n",
        "    \"\"\"\n",
        "    Extrait les d√©tails complets d'une offre d'emploi depuis aijobs.ai\n",
        "    \n",
        "    Cette fonction utilise nos fonctions utilitaires pour d√©tecter automatiquement\n",
        "    le type de contrat, le niveau d'exp√©rience et les technologies recherch√©es.\n",
        "    \"\"\"\n",
        "    if soup is None:\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de la r√©cup√©ration de {url}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    job_data = {\n",
        "        'job_title': None,\n",
        "        'company_name': None,\n",
        "        'location': None,\n",
        "        'work_mode': None,  # Remote/Hybrid/On-site (d√©tect√© automatiquement)\n",
        "        'experience_level': None,  # Junior/Mid/Senior (d√©tect√© automatiquement)\n",
        "        'salary_min': None,\n",
        "        'salary_max': None,\n",
        "        'tech_stack': [],  # Liste des technologies (d√©tect√©es automatiquement)\n",
        "        'job_description': None,\n",
        "        'job_type': None,  # Full Time, Part Time, etc.\n",
        "        'job_url': url\n",
        "    }\n",
        "    \n",
        "    # Extraction du titre depuis aijobs.ai\n",
        "    title_elem = soup.find(\"div\", class_=\"post-main-title2\")\n",
        "    if title_elem:\n",
        "        job_data['job_title'] = title_elem.get_text(strip=True)\n",
        "    \n",
        "    # Extraction de l'entreprise depuis aijobs.ai\n",
        "    # M√©thode 1 : Chercher le span avec \"at\"\n",
        "    company_elem = soup.find(\"span\", string=lambda x: x and \"at\" in str(x).lower())\n",
        "    if company_elem:\n",
        "        company_span = company_elem.find_next_sibling(\"span\")\n",
        "        if company_span:\n",
        "            job_data['company_name'] = company_span.get_text(strip=True)\n",
        "    \n",
        "    # M√©thode 2 : Chercher le lien de l'entreprise\n",
        "    if not job_data['company_name']:\n",
        "        company_link = soup.find(\"a\", href=re.compile(r\"/company/\"))\n",
        "        if company_link:\n",
        "            company_name_elem = company_link.find(\"span\", class_=\"tw-card-title\")\n",
        "            if company_name_elem:\n",
        "                job_data['company_name'] = company_name_elem.get_text(strip=True)\n",
        "    \n",
        "    # Extraction du type d'emploi (Full Time, etc.)\n",
        "    job_type_elem = soup.find(\"span\", class_=re.compile(r\"tw-bg-\\[#0BA02C\\]\"))\n",
        "    if job_type_elem:\n",
        "        job_data['job_type'] = job_type_elem.get_text(strip=True)\n",
        "    \n",
        "    # Extraction de la localisation depuis aijobs.ai\n",
        "    location_elem = soup.find(\"div\", class_=\"remote\")\n",
        "    if location_elem:\n",
        "        location_p = location_elem.find(\"p\", class_=\"tw-mb-0\")\n",
        "        if location_p:\n",
        "            job_data['location'] = location_p.get_text(strip=True)\n",
        "    \n",
        "    # Extraction de la description compl√®te\n",
        "    desc_container = soup.find(\"div\", class_=\"job-description-container\")\n",
        "    if desc_container:\n",
        "        description_text = desc_container.get_text(separator=' ', strip=True)\n",
        "    else:\n",
        "        # Fallback : prendre tout le body\n",
        "        body = soup.find('body')\n",
        "        if body:\n",
        "            description_text = body.get_text(separator=' ', strip=True)\n",
        "        else:\n",
        "            description_text = \"\"\n",
        "    \n",
        "    job_data['job_description'] = description_text[:5000]  # Limiter la taille\n",
        "    \n",
        "    # üéØ UTILISER NOS FONCTIONS UTILITAIRES pour d√©tecter automatiquement :\n",
        "    # C'est ce qui diff√©rencie ce projet - nous analysons le texte pour extraire des infos\n",
        "    \n",
        "    # D√©tecter le type de contrat (Remote/Hybrid/On-site)\n",
        "    job_data['work_mode'] = detect_work_mode(description_text)\n",
        "    \n",
        "    # D√©tecter le niveau d'exp√©rience requis\n",
        "    job_data['experience_level'] = extract_experience_level(description_text)\n",
        "    \n",
        "    # Extraire les technologies recherch√©es\n",
        "    job_data['tech_stack'] = extract_tech_stack(description_text)\n",
        "    \n",
        "    # Extraire la fourchette salariale\n",
        "    min_sal, max_sal = extract_salary_range(description_text)\n",
        "    job_data['salary_min'] = min_sal\n",
        "    job_data['salary_max'] = max_sal\n",
        "    \n",
        "    # Si pas de salaire trouv√© dans la description, chercher dans les √©l√©ments sp√©cifiques\n",
        "    if not min_sal and not max_sal:\n",
        "        # Chercher dans les sections de salaire\n",
        "        salary_section = soup.find(\"div\", string=re.compile(r\"Salary\", re.IGNORECASE))\n",
        "        if salary_section:\n",
        "            salary_text = salary_section.get_text()\n",
        "            min_sal, max_sal = extract_salary_range(salary_text)\n",
        "            job_data['salary_min'] = min_sal\n",
        "            job_data['salary_max'] = max_sal\n",
        "    \n",
        "    return job_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Scraping des URLs d'Emplois depuis Plusieurs Pays\n",
        "\n",
        "Nous allons scraper depuis aijobs.ai pour plusieurs pays :\n",
        "- üá∫üá∏ √âtats-Unis\n",
        "- üá¨üáß Royaume-Uni  \n",
        "- üá®üá¶ Canada\n",
        "\n",
        "Les r√©sultats de tous les pays seront combin√©s pour une analyse globale du march√©.\n",
        "\n",
        "**Note :** Cette section montre comment collecter les URLs d'emplois depuis plusieurs localisations.\n",
        "Vous devrez adapter les s√©lecteurs CSS selon le site que vous utilisez.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Collecte des URLs d'emplois depuis plusieurs pays...\n",
            "\n",
            "Pays √† scraper: üá∫üá∏ √âtats-Unis, üá¨üáß Royaume-Uni, üá®üá¶ Canada\n",
            "\n",
            "\n",
            "============================================================\n",
            "üåç üá∫üá∏ √âtats-Unis\n",
            "============================================================\n",
            "üìÑ Scraping page 1...\n",
            "  ‚úÖ 20 emplois trouv√©s sur cette page\n",
            "üìÑ Scraping page 2...\n",
            "  ‚úÖ 20 emplois trouv√©s sur cette page\n",
            "\n",
            "‚úÖ Total: 40 URLs d'emplois uniques collect√©es\n",
            "‚úÖ 40 URLs collect√©es pour üá∫üá∏ √âtats-Unis\n",
            "\n",
            "============================================================\n",
            "üåç üá¨üáß Royaume-Uni\n",
            "============================================================\n",
            "üìÑ Scraping page 1...\n",
            "  ‚úÖ 20 emplois trouv√©s sur cette page\n",
            "üìÑ Scraping page 2...\n",
            "  ‚úÖ 11 emplois trouv√©s sur cette page\n",
            "\n",
            "‚úÖ Total: 31 URLs d'emplois uniques collect√©es\n",
            "‚úÖ 31 URLs collect√©es pour üá¨üáß Royaume-Uni\n",
            "\n",
            "============================================================\n",
            "üåç üá®üá¶ Canada\n",
            "============================================================\n",
            "üìÑ Scraping page 1...\n",
            "  ‚úÖ 20 emplois trouv√©s sur cette page\n",
            "üìÑ Scraping page 2...\n",
            "  ‚úÖ 16 emplois trouv√©s sur cette page\n",
            "\n",
            "‚úÖ Total: 36 URLs d'emplois uniques collect√©es\n",
            "‚úÖ 36 URLs collect√©es pour üá®üá¶ Canada\n",
            "\n",
            "============================================================\n",
            "‚úÖ TOTAL: 107 URLs d'emplois collect√©es depuis 3 pays\n",
            "============================================================\n",
            "\n",
            "üìä Apr√®s suppression des doublons: 107 URLs uniques\n"
          ]
        }
      ],
      "source": [
        "def collect_job_urls_from_aijobs(max_pages=5, location=\"United%20States\"):\n",
        "    \"\"\"\n",
        "    Collecte les URLs des offres d'emploi depuis aijobs.ai\n",
        "    \n",
        "    Cette fonction utilise les s√©lecteurs sp√©cifiques d'aijobs.ai\n",
        "    \"\"\"\n",
        "    BASE_URL = \"https://aijobs.ai\"\n",
        "    job_urls = []\n",
        "    \n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        try:\n",
        "            # Construire l'URL de la page\n",
        "            url = f\"{BASE_URL}/engineer?location={location}&page={page_num}\"\n",
        "            \n",
        "            print(f\"üìÑ Scraping page {page_num}...\")\n",
        "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            \n",
        "            # Trouver toutes les cartes d'emploi (s√©lecteur sp√©cifique d'aijobs.ai)\n",
        "            job_cards = soup.find_all(\"a\", class_=\"jobcardStyle1\")\n",
        "            \n",
        "            # Extraire les URLs\n",
        "            page_job_urls = []\n",
        "            for card in job_cards:\n",
        "                href = card.get(\"href\")\n",
        "                if href:\n",
        "                    # Convertir en URL absolue si n√©cessaire\n",
        "                    if href.startswith('/'):\n",
        "                        from urllib.parse import urljoin\n",
        "                        href = urljoin(BASE_URL, href)\n",
        "                    if href not in page_job_urls:\n",
        "                        page_job_urls.append(href)\n",
        "            \n",
        "            job_urls.extend(page_job_urls)\n",
        "            print(f\"  ‚úÖ {len(page_job_urls)} emplois trouv√©s sur cette page\")\n",
        "            \n",
        "            # D√©lai entre les requ√™tes pour respecter le serveur\n",
        "            time.sleep(REQUEST_DELAY)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur sur la page {page_num}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Supprimer les doublons\n",
        "    unique_urls = list(set(job_urls))\n",
        "    print(f\"\\n‚úÖ Total: {len(unique_urls)} URLs d'emplois uniques collect√©es\")\n",
        "    return unique_urls\n",
        "\n",
        "# Configuration des pays √† scraper\n",
        "COUNTRIES_TO_SCRAPE = {\n",
        "    \"üá∫üá∏ √âtats-Unis\": \"United%20States\",\n",
        "    \"üá¨üáß Royaume-Uni\": \"United%20Kingdom\",\n",
        "    \"üá®üá¶ Canada\": \"Canada\"\n",
        "}\n",
        "\n",
        "# Collecter les URLs d'emplois depuis tous les pays\n",
        "print(\"üîç Collecte des URLs d'emplois depuis plusieurs pays...\\n\")\n",
        "print(f\"Pays √† scraper: {', '.join(COUNTRIES_TO_SCRAPE.keys())}\\n\")\n",
        "\n",
        "all_job_urls = {}\n",
        "total_urls = 0\n",
        "\n",
        "for country_name, location_code in COUNTRIES_TO_SCRAPE.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üåç {country_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    job_urls = collect_job_urls_from_aijobs(max_pages=2, location=location_code)\n",
        "    all_job_urls[country_name] = job_urls\n",
        "    total_urls += len(job_urls)\n",
        "    print(f\"‚úÖ {len(job_urls)} URLs collect√©es pour {country_name}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ TOTAL: {total_urls} URLs d'emplois collect√©es depuis {len(COUNTRIES_TO_SCRAPE)} pays\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Combiner toutes les URLs en une seule liste\n",
        "job_urls = []\n",
        "for country_name, urls in all_job_urls.items():\n",
        "    job_urls.extend(urls)\n",
        "\n",
        "# Supprimer les doublons (au cas o√π un m√™me emploi appara√Ætrait dans plusieurs pays)\n",
        "job_urls = list(set(job_urls))\n",
        "print(f\"\\nüìä Apr√®s suppression des doublons: {len(job_urls)} URLs uniques\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extraction des D√©tails de Chaque Emploi\n",
        "\n",
        "Maintenant, nous extrayons les d√©tails complets de chaque emploi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Extraction des d√©tails des emplois...\n",
            "\n",
            "  [1/50] https://aijobs.ai/job/machine-learning-engineer-intern-8...\n",
            "      ‚úÖ Machine Learning Engineer Intern\n",
            "  [2/50] https://aijobs.ai/job/ai-software-engineer-2...\n",
            "      ‚úÖ AI Software Engineer\n",
            "  [3/50] https://aijobs.ai/job/senior-ai-engineer-ai-labs...\n",
            "      ‚úÖ Senior AI Engineer, AI Labs\n",
            "  [4/50] https://aijobs.ai/job/aiml-engineer-solution-designer-toront...\n",
            "      ‚úÖ AI/ML Engineer / Solution Designer ‚Äì Toronto - Hyb\n",
            "  [5/50] https://aijobs.ai/job/machine-learning-engineer-2462...\n",
            "      ‚úÖ Machine Learning Engineer\n",
            "  [6/50] https://aijobs.ai/job/sr-machine-learning-engineer-1...\n",
            "      ‚úÖ Sr. Machine Learning Engineer\n",
            "  [7/50] https://aijobs.ai/job/backend-software-engineer-python...\n",
            "      ‚úÖ Backend Software Engineer - Python\n",
            "  [8/50] https://aijobs.ai/job/senior-applied-ai-engineer-fmd...\n",
            "      ‚úÖ Senior Applied AI Engineer (f/m/d)\n",
            "  [9/50] https://aijobs.ai/job/staff-post-silicon-validation-engineer...\n",
            "      ‚úÖ Staff Post Silicon Validation Engineer (Bringup)\n",
            "  [10/50] https://aijobs.ai/job/software-engineer-i-ai-platform...\n",
            "      ‚úÖ Software Engineer I - AI Platform\n",
            "  [11/50] https://aijobs.ai/job/engineering-manager-37...\n",
            "      ‚úÖ Engineering Manager\n",
            "  [12/50] https://aijobs.ai/job/staff-engineer-aiml-for-circuit-design...\n",
            "      ‚úÖ Staff Engineer, AI/ML for Circuit Design\n",
            "  [13/50] https://aijobs.ai/job/senior-software-engineer-mlgraphics...\n",
            "      ‚úÖ Senior Software Engineer (ML/Graphics)\n",
            "  [14/50] https://aijobs.ai/job/director-of-engineering-ai-ml-data-col...\n",
            "      ‚úÖ Director of Engineering, AI & ML, Data Collections\n",
            "  [15/50] https://aijobs.ai/job/senior-software-engineer-ai-data...\n",
            "      ‚úÖ Senior Software Engineer, AI Data\n",
            "  [16/50] https://aijobs.ai/job/staff-machine-learning-engineer-445...\n",
            "      ‚úÖ Staff Machine Learning Engineer\n",
            "  [17/50] https://aijobs.ai/job/senior-directorvp-head-of-ai-engineeri...\n",
            "      ‚úÖ Senior Director/VP - Head of AI Engineering\n",
            "  [18/50] https://aijobs.ai/job/senior-ai-engineer-437...\n",
            "      ‚úÖ Senior AI Engineer\n",
            "  [19/50] https://aijobs.ai/job/senior-machine-learning-engineer-1772...\n",
            "      ‚úÖ Senior Machine Learning Engineer\n",
            "  [20/50] https://aijobs.ai/job/staff-machine-learning-engineer-444...\n",
            "      ‚úÖ Staff Machine Learning Engineer\n",
            "  [21/50] https://aijobs.ai/job/senior-software-engineer-i-3...\n",
            "      ‚úÖ Senior Software Engineer I\n",
            "  [22/50] https://aijobs.ai/job/llm-inference-performance-evals-engine...\n",
            "      ‚úÖ LLM‚ÄØInference Performance‚ÄØ&‚ÄØEvals‚ÄØEngineer\n",
            "  [23/50] https://aijobs.ai/job/machine-learning-performance-engineer-...\n",
            "      ‚úÖ Machine Learning Performance Engineer\n",
            "  [24/50] https://aijobs.ai/job/sr-machine-learning-engineer...\n",
            "      ‚úÖ Sr. Machine Learning Engineer\n",
            "  [25/50] https://aijobs.ai/job/senior-machine-learning-engineer-1773...\n",
            "      ‚úÖ Senior Machine Learning Engineer\n",
            "  [26/50] https://aijobs.ai/job/senior-software-engineer-ml-platform...\n",
            "      ‚úÖ Senior Software Engineer, ML Platform\n",
            "  [27/50] https://aijobs.ai/job/senior-embedded-swfw-engineer-bringup...\n",
            "      ‚úÖ Senior Embedded SW/FW Engineer (Bringup)\n",
            "  [28/50] https://aijobs.ai/job/forward-deployed-engineer-new-jersey...\n",
            "      ‚úÖ Forward Deployed Engineer - New Jersey\n",
            "  [29/50] https://aijobs.ai/job/senior-runtime-performance-engineer...\n",
            "      ‚úÖ Senior Runtime Performance Engineer\n",
            "  [30/50] https://aijobs.ai/job/senior-software-engineer-ai-131...\n",
            "      ‚úÖ Senior Software Engineer - AI\n",
            "  [31/50] https://aijobs.ai/job/frontend-engineer-114...\n",
            "      ‚úÖ Frontend Engineer\n",
            "  [32/50] https://aijobs.ai/job/full-stack-llm-engineer...\n",
            "      ‚úÖ Full Stack LLM Engineer\n",
            "  [33/50] https://aijobs.ai/job/principal-post-silicon-validation-engi...\n",
            "      ‚úÖ Principal Post Silicon Validation Engineer (Bringu\n",
            "  [34/50] https://aijobs.ai/job/director-of-engineering-platform-servi...\n",
            "      ‚úÖ Director of Engineering - Platform Services\n",
            "  [35/50] https://aijobs.ai/job/software-engineer-ii-36...\n",
            "      ‚úÖ Software Engineer II\n",
            "  [36/50] https://aijobs.ai/job/principal-engineer-ai-compiler...\n",
            "      ‚úÖ Principal Engineer, AI Compiler\n",
            "  [37/50] https://aijobs.ai/job/director-of-ai-engineering...\n",
            "      ‚úÖ Director of AI Engineering\n",
            "  [38/50] https://aijobs.ai/job/software-engineer-autonomous-vehicles...\n",
            "      ‚úÖ Software Engineer, Autonomous Vehicles\n",
            "  [39/50] https://aijobs.ai/job/sr-engineer-soc-design-verification-ai...\n",
            "      ‚úÖ Sr. Engineer, SoC Design Verification ‚Äì AI/ML Acce\n",
            "  [40/50] https://aijobs.ai/job/principal-embedded-swfw-engineer-bring...\n",
            "      ‚úÖ Principal Embedded SW/FW Engineer (Bringup)\n",
            "  [41/50] https://aijobs.ai/job/machine-learning-engineer-2480...\n",
            "      ‚úÖ Machine Learning Engineer\n",
            "  [42/50] https://aijobs.ai/job/senior-software-engineer-ai-applicatio...\n",
            "      ‚úÖ Senior Software Engineer - AI Applications\n",
            "  [43/50] https://aijobs.ai/job/machine-learning-engineer-ll...\n",
            "      ‚úÖ Machine Learning Engineer ll\n",
            "  [44/50] https://aijobs.ai/job/senior-lead-machine-learning-engineer-...\n",
            "      ‚úÖ Senior Lead Machine Learning Engineer, Agentic AI\n",
            "  [45/50] https://aijobs.ai/job/associate-machine-learning-engineer-1...\n",
            "      ‚úÖ Associate Machine Learning Engineer\n",
            "  [46/50] https://aijobs.ai/job/senior-engineering-manager-ai-platform...\n",
            "      ‚úÖ Senior Engineering Manager - AI Platform (f/m/d)\n",
            "  [47/50] https://aijobs.ai/job/applied-data-center-design-engineer...\n",
            "      ‚úÖ Applied Data Center Design Engineer\n",
            "  [48/50] https://aijobs.ai/job/engineering-manager-ai-business-servic...\n",
            "      ‚úÖ Engineering Manager - AI Business Services (f/m/d)\n",
            "  [49/50] https://aijobs.ai/job/senior-software-engineer-ai-net-platfo...\n",
            "      ‚úÖ Senior Software Engineer - AI & .NET Platforms - b\n",
            "  [50/50] https://aijobs.ai/job/staff-machine-learning-engineer-data-c...\n",
            "      ‚úÖ Staff Machine Learning Engineer, Data Collections \n",
            "\n",
            "‚úÖ 50 emplois extraits avec succ√®s\n"
          ]
        }
      ],
      "source": [
        "# Extraire les d√©tails de chaque emploi\n",
        "print(\"\\nüìã Extraction des d√©tails des emplois...\\n\")\n",
        "\n",
        "all_jobs = []\n",
        "\n",
        "for i, job_url in enumerate(job_urls[:MAX_JOBS_TO_SCRAPE], 1):\n",
        "    print(f\"  [{i}/{min(MAX_JOBS_TO_SCRAPE, len(job_urls))}] {job_url[:60]}...\")\n",
        "    \n",
        "    try:\n",
        "        job_data = extract_job_details_from_aijobs(job_url)\n",
        "        if job_data and job_data.get('job_title'):\n",
        "            all_jobs.append(job_data)\n",
        "            print(f\"      ‚úÖ {job_data.get('job_title', 'N/A')[:50]}\")\n",
        "        else:\n",
        "            print(f\"      ‚ö†Ô∏è Donn√©es incompl√®tes\")\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ùå Erreur: {e}\")\n",
        "    \n",
        "    # D√©lai entre les requ√™tes\n",
        "    time.sleep(REQUEST_DELAY)\n",
        "\n",
        "print(f\"\\n‚úÖ {len(all_jobs)} emplois extraits avec succ√®s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Traitement et Nettoyage des Donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Aper√ßu des donn√©es:\n",
            "                                           job_title  company_name  \\\n",
            "0                   Machine Learning Engineer Intern        Moloco   \n",
            "1                               AI Software Engineer       Anaplan   \n",
            "2                        Senior AI Engineer, AI Labs           NPR   \n",
            "3  AI/ML Engineer / Solution Designer ‚Äì Toronto -...         Capco   \n",
            "4                          Machine Learning Engineer  Speechmatics   \n",
            "\n",
            "                                            location work_mode  \\\n",
            "0                  London,  England,  United Kingdom   On-site   \n",
            "1                        Manchester,  United Kingdom    Hybrid   \n",
            "2  Washington,  District of Columbia,  United States    Hybrid   \n",
            "3                                   Canada - Toronto    Hybrid   \n",
            "4               Cambridge,  England,  United Kingdom    Hybrid   \n",
            "\n",
            "  experience_level  salary_min  salary_max  \\\n",
            "0           Junior         NaN         NaN   \n",
            "1           Senior         NaN         NaN   \n",
            "2           Senior         0.0    186000.0   \n",
            "3           Senior         NaN         NaN   \n",
            "4           Senior      2000.0      3000.0   \n",
            "\n",
            "                                       tech_stack  \\\n",
            "0  [Go, Kubernetes, Machine Learning, TensorFlow]   \n",
            "1             [Python, CI/CD, React, Java, Kafka]   \n",
            "2     [AWS, Python, CI/CD, GCP, Machine Learning]   \n",
            "3                           [TensorFlow, PyTorch]   \n",
            "4                          [Go, Machine Learning]   \n",
            "\n",
            "                                     job_description   job_type  \\\n",
            "0  About Moloco: Moloco builds some of the most p...  Full Time   \n",
            "1  At Anaplan, we are a team of innovators focuse...  Full Time   \n",
            "2  OVERVIEW A thriving, mission-driven multimedia...  Full Time   \n",
            "3  AI/ML Engineer / Solution Designer ‚Äì Toronto -...  Full Time   \n",
            "4  Speechmatics is a cutting-edge applied AI Rese...  Full Time   \n",
            "\n",
            "                                             job_url  \\\n",
            "0  https://aijobs.ai/job/machine-learning-enginee...   \n",
            "1       https://aijobs.ai/job/ai-software-engineer-2   \n",
            "2   https://aijobs.ai/job/senior-ai-engineer-ai-labs   \n",
            "3  https://aijobs.ai/job/aiml-engineer-solution-d...   \n",
            "4  https://aijobs.ai/job/machine-learning-enginee...   \n",
            "\n",
            "                                 tech_stack_str  avg_salary  \n",
            "0  Go, Kubernetes, Machine Learning, TensorFlow         NaN  \n",
            "1             Python, CI/CD, React, Java, Kafka         NaN  \n",
            "2     AWS, Python, CI/CD, GCP, Machine Learning     93000.0  \n",
            "3                           TensorFlow, PyTorch         NaN  \n",
            "4                          Go, Machine Learning      2500.0  \n",
            "\n",
            "üìà Statistiques:\n",
            "  - Total d'emplois: 50\n",
            "  - Types de contrats: {'Hybrid': 25, 'Remote': 13, 'Non sp√©cifi√©': 7, 'On-site': 5}\n",
            "  - Niveaux d'exp√©rience: {'Senior': 31, 'Lead/Principal': 11, 'Junior': 6, 'Non sp√©cifi√©': 1, 'Mid-level': 1}\n",
            "  - Salaire moyen: ‚Ç¨113,931\n"
          ]
        }
      ],
      "source": [
        "# Convertir en DataFrame\n",
        "df = pd.DataFrame(all_jobs)\n",
        "\n",
        "# Nettoyer les donn√©es\n",
        "if not df.empty:\n",
        "    # Convertir tech_stack en string pour le CSV (on peut le r√©analyser plus tard)\n",
        "    df['tech_stack_str'] = df['tech_stack'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
        "    \n",
        "    # Calculer le salaire moyen si min et max sont disponibles\n",
        "    df['avg_salary'] = df.apply(\n",
        "        lambda row: (row['salary_min'] + row['salary_max']) / 2 \n",
        "        if pd.notna(row['salary_min']) and pd.notna(row['salary_max']) \n",
        "        else None, axis=1\n",
        "    )\n",
        "    \n",
        "    # Nettoyer les localisations\n",
        "    df['location'] = df['location'].fillna('Non sp√©cifi√©e')\n",
        "    \n",
        "    print(\"üìä Aper√ßu des donn√©es:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nüìà Statistiques:\")\n",
        "    print(f\"  - Total d'emplois: {len(df)}\")\n",
        "    print(f\"  - Types de contrats: {df['work_mode'].value_counts().to_dict()}\")\n",
        "    print(f\"  - Niveaux d'exp√©rience: {df['experience_level'].value_counts().to_dict()}\")\n",
        "    \n",
        "    if 'avg_salary' in df.columns:\n",
        "        avg_sal = df['avg_salary'].dropna()\n",
        "        if len(avg_sal) > 0:\n",
        "            print(f\"  - Salaire moyen: ‚Ç¨{avg_sal.mean():,.0f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Aucune donn√©e disponible\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analyse des Technologies les Plus Demand√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Top 10 Technologies les Plus Demand√©es:\n",
            "  Python: 34 offres\n",
            "  Machine Learning: 27 offres\n",
            "  PyTorch: 17 offres\n",
            "  TensorFlow: 15 offres\n",
            "  Go: 11 offres\n",
            "  AWS: 9 offres\n",
            "  Java: 8 offres\n",
            "  Kubernetes: 7 offres\n",
            "  CI/CD: 7 offres\n",
            "  Docker: 7 offres\n"
          ]
        }
      ],
      "source": [
        "# Analyser les technologies les plus recherch√©es\n",
        "if not df.empty and 'tech_stack' in df.columns:\n",
        "    all_techs = []\n",
        "    for tech_list in df['tech_stack']:\n",
        "        if isinstance(tech_list, list):\n",
        "            all_techs.extend(tech_list)\n",
        "    \n",
        "    tech_counter = Counter(all_techs)\n",
        "    top_techs = tech_counter.most_common(10)\n",
        "    \n",
        "    print(\"üîß Top 10 Technologies les Plus Demand√©es:\")\n",
        "    for tech, count in top_techs:\n",
        "        print(f\"  {tech}: {count} offres\")\n",
        "    \n",
        "    # Ajouter cette info au DataFrame\n",
        "    df['top_tech_count'] = df['tech_stack'].apply(\n",
        "        lambda x: len([t for t in x if t in dict(top_techs[:5])]) if isinstance(x, list) else 0\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Sauvegarde des Donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Donn√©es sauvegard√©es dans data/donnees_marche_emploi.csv\n",
            "   - 50 lignes\n",
            "   - 11 colonnes\n"
          ]
        }
      ],
      "source": [
        "# Pr√©parer le DataFrame pour la sauvegarde\n",
        "if not df.empty:\n",
        "    # S√©lectionner les colonnes √† sauvegarder (inclure country_origin si disponible)\n",
        "    columns_to_save = [\n",
        "        'job_title', 'company_name', 'location', 'work_mode', \n",
        "        'experience_level', 'salary_min', 'salary_max', 'avg_salary',\n",
        "        'tech_stack_str', 'job_description', 'job_url'\n",
        "    ]\n",
        "    \n",
        "    # Ajouter country_origin si disponible\n",
        "    if 'country_origin' in df.columns:\n",
        "        columns_to_save.insert(3, 'country_origin')  # Apr√®s location\n",
        "    \n",
        "    # Garder seulement les colonnes qui existent\n",
        "    save_df = df[[col for col in columns_to_save if col in df.columns]].copy()\n",
        "    \n",
        "    # Sauvegarder en CSV\n",
        "    output_file = 'data/donnees_marche_emploi.csv'\n",
        "    save_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "    print(f\"\\nüíæ Donn√©es sauvegard√©es dans {output_file}\")\n",
        "    print(f\"   - {len(save_df)} lignes\")\n",
        "    print(f\"   - {len(save_df.columns)} colonnes\")\n",
        "    \n",
        "    # Afficher la r√©partition finale par pays\n",
        "    if 'country_origin' in save_df.columns:\n",
        "        print(f\"\\nüåç R√©partition finale par pays:\")\n",
        "        country_final = save_df['country_origin'].value_counts()\n",
        "        for country, count in country_final.items():\n",
        "            percentage = (count / len(save_df)) * 100\n",
        "            print(f\"   {country}: {count} offres ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Aucune donn√©e √† sauvegarder\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
