# üìñ Guide d'Utilisation Complet

> Guide d√©taill√© pour utiliser le projet d'analyse de march√© des emplois tech

## üìã Table des Mati√®res

- [D√©marrage Rapide](#-d√©marrage-rapide)
- [Installation D√©taill√©e](#-installation-d√©taill√©e)
- [Partie 1 : Scraping des Donn√©es](#-partie-1--scraping-des-donn√©es)
- [Partie 2 : Dashboard Interactif](#-partie-2--dashboard-interactif)
- [Notes P√©dagogiques](#-notes-p√©dagogiques)
- [Exercices Sugg√©r√©s](#-exercices-sugg√©r√©s)
- [R√©solution de Probl√®mes](#-r√©solution-de-probl√®mes)
- [Ressources Compl√©mentaires](#-ressources-compl√©mentaires)

---

## üöÄ D√©marrage Rapide

### Pr√©requis

Avant de commencer, assurez-vous d'avoir :

- ‚úÖ Python 3.8 ou sup√©rieur install√©
- ‚úÖ Une connexion Internet active
- ‚úÖ Un navigateur web moderne (Chrome, Firefox, Safari, Edge)

### Installation Express (3 minutes)

```bash
# 1. Installer les d√©pendances
uv sync
# ou
pip install -r requirements.txt

# 2. Lancer Jupyter Notebook
uv run jupyter notebook
# ou
jupyter notebook

# 3. Ouvrir le notebook Partie1-scraper_emplois.ipynb
# 4. Ex√©cuter toutes les cellules (Shift + Enter)

# 5. Lancer le dashboard
streamlit run Partie2_tableau_bord_marche_emploi.py
```

---

## üì¶ Installation D√©taill√©e

### √âtape 1 : V√©rifier Python

```bash
python --version
# Doit afficher Python 3.8.x ou sup√©rieur
```

Si Python n'est pas install√©, t√©l√©chargez-le depuis [python.org](https://www.python.org/downloads/)

### √âtape 2 : Cr√©er un environnement virtuel (Recommand√©)

```bash
# Cr√©er l'environnement
python -m venv venv

# Activer l'environnement
# Sur macOS/Linux :
source venv/bin/activate
# Sur Windows :
venv\Scripts\activate
```

### √âtape 3 : Installer les d√©pendances

#### Option A : Avec uv (Recommand√© - Plus rapide)

```bash
# Installer uv si ce n'est pas d√©j√† fait
pip install uv

# Installer toutes les d√©pendances
uv sync
```

#### Option B : Avec pip (Standard)

```bash
pip install -r requirements.txt
```

### √âtape 4 : V√©rifier l'installation

```bash
python -c "import pandas, streamlit, plotly, bs4; print('‚úÖ Installation r√©ussie !')"
```

Si vous voyez le message de succ√®s, vous √™tes pr√™t √† commencer !

---

## üìì Partie 1 : Scraping des Donn√©es

### Vue d'ensemble

Le notebook `Partie1-scraper_emplois.ipynb` contient tout le code n√©cessaire pour :

1. Scraper des offres d'emploi depuis des sites sp√©cialis√©s
2. Extraire automatiquement des informations structur√©es
3. Nettoyer et traiter les donn√©es
4. Sauvegarder les r√©sultats en CSV

### Structure du Notebook

Le notebook est organis√© en sections :

1. **Imports et Configuration** : Biblioth√®ques et param√®tres
2. **Fonctions Utilitaires** : Extraction intelligente de donn√©es
3. **Fonction Principale** : Scraping depuis un site sp√©cifique
4. **Collecte des URLs** : R√©cup√©ration des liens d'emplois
5. **Extraction des D√©tails** : R√©cup√©ration des informations compl√®tes
6. **Traitement des Donn√©es** : Nettoyage et normalisation
7. **Analyse** : Statistiques et tendances
8. **Sauvegarde** : Export en CSV

### Instructions Pas √† Pas

#### 1. Ouvrir Jupyter Notebook

```bash
uv run jupyter notebook
# ou
jupyter notebook
```

Jupyter Notebook s'ouvrira dans votre navigateur par d√©faut.

#### 2. Ouvrir le Notebook

Dans Jupyter Notebook, naviguez vers `Partie1-scraper_emplois.ipynb` et cliquez pour l'ouvrir.

#### 3. Ex√©cuter les Cellules

**M√©thode 1 : Cellule par cellule**
- Cliquez sur une cellule
- Appuyez sur `Shift + Enter` pour ex√©cuter
- Passez √† la cellule suivante

**M√©thode 2 : Tout ex√©cuter**
- Menu : `Cell` ‚Üí `Run All`
- Ou raccourci clavier : `Ctrl + Shift + Enter` (Windows/Linux) / `Cmd + Shift + Enter` (Mac)

#### 4. Comprendre les R√©sultats

Apr√®s l'ex√©cution, vous verrez :

- üìä Nombre d'emplois scrap√©s
- üìà Statistiques sur les types de contrats
- üîß Technologies les plus recherch√©es
- üí∞ Informations sur les salaires
- üíæ Confirmation de sauvegarde dans `data/donnees_marche_emploi.csv`

### Fonctionnalit√©s Avanc√©es

#### Extraction Intelligente

Le notebook utilise des fonctions pour d√©tecter automatiquement :

- **Types de contrats** : Analyse du texte pour identifier Remote/Hybrid/On-site
- **Niveaux d'exp√©rience** : D√©tection de Junior/Mid-level/Senior depuis les descriptions
- **Stack technique** : Identification des technologies mentionn√©es
- **Salaires** : Extraction des fourchettes salariales

#### Personnalisation

Pour adapter le scraping √† un autre site :

1. **Modifier les s√©lecteurs CSS** dans `extract_job_details_from_aijobs()`
2. **Inspecter le HTML** du site cible avec les outils d√©veloppeur
3. **Tester sur une seule page** avant de scraper en masse
4. **Adapter les fonctions utilitaires** si n√©cessaire

### ‚ö†Ô∏è Points d'Attention

#### Respect des Sites Web

- ‚úÖ **V√©rifiez robots.txt** : `https://site.com/robots.txt`
- ‚úÖ **D√©lais entre requ√™tes** : Minimum 2 secondes (configur√© dans `REQUEST_DELAY`)
- ‚úÖ **User-Agent appropri√©** : D√©j√† configur√© dans `HEADERS`
- ‚úÖ **Respectez les limites** : Ne scrapez pas trop de pages d'un coup

#### Gestion des Erreurs

Le code inclut des `try/except` pour g√©rer :
- Les erreurs de connexion
- Les pages non trouv√©es
- Les changements de structure HTML
- Les timeouts

Si vous rencontrez des erreurs :
1. V√©rifiez les messages dans la sortie du notebook
2. Inspectez le code HTML du site
3. Adaptez les s√©lecteurs CSS si n√©cessaire

---

## üé® Partie 2 : Dashboard Interactif

### Vue d'ensemble

Le dashboard Streamlit (`Partie2_tableau_bord_marche_emploi.py`) offre une interface web interactive pour :

- Visualiser les donn√©es scrap√©es
- Filtrer et rechercher des emplois
- Analyser les tendances du march√©
- Exporter les donn√©es filtr√©es

### Lancer le Dashboard

```bash
streamlit run Partie2_tableau_bord_marche_emploi.py
```

Le dashboard s'ouvrira automatiquement dans votre navigateur √† `http://localhost:8501`

### Utiliser le Dashboard

#### 1. Charger les Donn√©es

Le dashboard propose deux options :

- **üìÅ Donn√©es existantes** : Charge `data/donnees_marche_emploi.csv`
- **üåê Scraping en direct** : Scrape des donn√©es directement depuis le dashboard

#### 2. Naviguer dans les Sections

Le dashboard est organis√© en onglets :

- **üìä Vue d'ensemble** : Statistiques globales
- **üåç G√©ographie** : R√©partition par localisation
- **üíº Types de contrats** : Analyse Remote/Hybrid/On-site
- **üìà Exp√©rience** : Distribution par niveau requis
- **üîß Technologies** : Stack technique recherch√©e
- **üí∞ Salaires** : Analyse des r√©mun√©rations

#### 3. Utiliser les Filtres

- **Recherche textuelle** : Rechercher dans les titres, entreprises, descriptions
- **Filtres par crit√®res** : Localisation, type de contrat, niveau d'exp√©rience
- **Filtres par technologies** : S√©lectionner les technologies recherch√©es

#### 4. Exporter les Donn√©es

- Cliquez sur le bouton **üì• T√©l√©charger CSV**
- Les donn√©es filtr√©es seront t√©l√©charg√©es
- Ouvrez le fichier dans Excel, Google Sheets, ou un autre outil

### Fonctionnalit√©s du Dashboard

#### Visualisations Interactives

- **Graphiques Plotly** : Zoom, pan, hover pour plus de d√©tails
- **Tableaux interactifs** : Tri et filtrage dans les tableaux
- **Cartes g√©ographiques** : Visualisation des opportunit√©s par r√©gion

#### Analyses Disponibles

- üìä **Statistiques globales** : Nombre total d'emplois, salaires moyens
- üåç **Analyse g√©ographique** : Top villes/r√©gions avec le plus d'opportunit√©s
- üíº **Types de contrats** : Pourcentage Remote vs Hybrid vs On-site
- üìà **Niveaux d'exp√©rience** : Distribution Junior/Mid/Senior
- üîß **Technologies** : Top 10 technologies les plus recherch√©es
- üí∞ **Salaires** : Comparaison par localisation et type de contrat

### Astuces d'Utilisation

- üí° **Combinez les filtres** : Utilisez plusieurs filtres simultan√©ment pour des analyses pr√©cises
- üí° **Explorez les graphiques** : Passez la souris sur les √©l√©ments pour voir les d√©tails
- üí° **Exportez r√©guli√®rement** : Sauvegardez vos analyses filtr√©es
- üí° **Comparez les p√©riodes** : Scrapez √† diff√©rents moments pour voir l'√©volution

---

## üéì Notes P√©dagogiques

### Pour les √âtudiants

Ce projet est con√ßu pour vous apprendre :

#### Concepts Cl√©s

1. **Web Scraping**
   - Comprendre la structure HTML
   - Utiliser des s√©lecteurs CSS/XPath
   - G√©rer les requ√™tes HTTP
   - Respecter l'√©thique du scraping

2. **Traitement de Donn√©es**
   - Nettoyer des donn√©es brutes
   - Normaliser les formats
   - G√©rer les valeurs manquantes
   - Transformer les donn√©es

3. **Visualisation**
   - Cr√©er des graphiques interactifs
   - Concevoir des dashboards
   - Pr√©senter des donn√©es de mani√®re claire

4. **Analyse de Donn√©es**
   - Identifier des tendances
   - Faire des statistiques descriptives
   - Comparer des groupes de donn√©es

### Points d'Attention Importants

#### 1. Respect des Sites Web

- ‚ö†Ô∏è **Toujours v√©rifier robots.txt** avant de scraper
- ‚ö†Ô∏è **Ajouter des d√©lais** entre les requ√™tes (minimum 2 secondes)
- ‚ö†Ô∏è **Ne pas surcharger** les serveurs
- ‚ö†Ô∏è **Respecter les conditions d'utilisation** des sites

#### 2. Gestion des Erreurs

- ‚úÖ Le code inclut des `try/except` pour g√©rer les erreurs
- ‚úÖ V√©rifiez les messages d'erreur si quelque chose ne fonctionne pas
- ‚úÖ Les sites peuvent changer leur structure HTML
- ‚úÖ Adaptez les s√©lecteurs CSS si n√©cessaire

#### 3. Adaptation du Code

- üîß Les s√©lecteurs CSS/HTML doivent √™tre adapt√©s selon le site
- üîß Inspectez le code HTML avec les outils d√©veloppeur (F12)
- üîß Testez d'abord sur une seule page avant de scraper en masse
- üîß Documentez vos modifications

### Fonctionnalit√©s Uniques de ce Projet

Ce projet se distingue par son focus sur :

- **Types de contrats** : Analyse d√©taill√©e Remote/Hybrid/On-site
- **Niveaux d'exp√©rience** : Extraction automatique depuis les descriptions
- **Stack technique** : D√©tection intelligente des technologies recherch√©es
- **Analyse de march√©** : Tendances g√©ographiques et salariales
- **Extraction intelligente** : Utilisation de regex et NLP pour extraire des infos structur√©es

---

## üí° Exercices Sugg√©r√©s

### Niveau D√©butant üü¢

#### Exercice 1 : Modifier les Fonctions d'Extraction

**Objectif** : Ajouter la d√©tection d'une nouvelle information

**T√¢ches** :
1. Cr√©er une fonction `extract_contract_type()` pour d√©tecter CDI/CDD/Freelance
2. Ajouter cette fonction dans le pipeline d'extraction
3. Tester sur quelques descriptions d'emploi

**Indices** :
- Utilisez des mots-cl√©s comme "CDI", "permanent", "freelance", "contract"
- Suivez le pattern des fonctions existantes (`detect_work_mode()`)

#### Exercice 2 : Ajouter des Technologies

**Objectif** : √âtendre la liste des technologies d√©tect√©es

**T√¢ches** :
1. Ouvrir la fonction `extract_tech_stack()`
2. Ajouter 5 nouvelles technologies √† la liste `tech_keywords`
3. Tester sur des descriptions contenant ces technologies

**Exemples** : Vue.js, Svelte, Rust, Elixir, GraphQL

#### Exercice 3 : Personnaliser les Graphiques

**Objectif** : Modifier les couleurs et styles des visualisations

**T√¢ches** :
1. Ouvrir le dashboard Streamlit
2. Trouver les sections de cr√©ation de graphiques
3. Modifier les couleurs dans les param√®tres `color_discrete_map`
4. Changer les titres et labels

### Niveau Interm√©diaire üü°

#### Exercice 1 : Adapter pour un Autre Site

**Objectif** : Scraper depuis un site d'emploi diff√©rent

**T√¢ches** :
1. Choisir un site d'emploi tech (ex: Indeed, LinkedIn, etc.)
2. Inspecter le HTML avec les outils d√©veloppeur
3. Cr√©er une nouvelle fonction `extract_job_details_from_[site]()`
4. Adapter les s√©lecteurs CSS
5. Tester sur quelques pages

**Points d'attention** :
- Respectez robots.txt
- Ajoutez des d√©lais appropri√©s
- G√©rez les erreurs

#### Exercice 2 : Analyse de Sentiment

**Objectif** : Analyser le ton des descriptions d'emploi

**T√¢ches** :
1. Installer une biblioth√®que de sentiment analysis (ex: `textblob` ou `vaderSentiment`)
2. Cr√©er une fonction pour analyser le sentiment
3. Ajouter une colonne "sentiment" au DataFrame
4. Visualiser les r√©sultats dans le dashboard

**Indices** :
```python
from textblob import TextBlob

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity  # -1 √† 1
```

#### Exercice 3 : Graphiques de Corr√©lation

**Objectif** : Cr√©er des graphiques montrant les corr√©lations

**T√¢ches** :
1. Analyser la corr√©lation entre salaire et technologies
2. Cr√©er un graphique de corr√©lation (heatmap)
3. Analyser la corr√©lation entre type de contrat et localisation
4. Ajouter ces visualisations au dashboard

**Exemples de corr√©lations** :
- Salaire vs Nombre de technologies requises
- Remote vs Salaires moyens
- Technologies vs Niveau d'exp√©rience

### Niveau Avanc√© üî¥

#### Exercice 1 : Scraping Multi-Sites Simultan√©

**Objectif** : Scraper plusieurs sites en parall√®le

**T√¢ches** :
1. Utiliser `concurrent.futures` ou `asyncio`
2. Cr√©er des fonctions de scraping pour 2-3 sites diff√©rents
3. Ex√©cuter le scraping en parall√®le
4. Combiner les r√©sultats

**Points d'attention** :
- G√©rez les erreurs pour chaque site
- Respectez les limites de chaque site
- Normalisez les donn√©es de diff√©rents sites

#### Exercice 2 : Comparaison de Salaires par R√©gion

**Objectif** : Cr√©er un syst√®me de comparaison avanc√©

**T√¢ches** :
1. Extraire les villes/pays depuis les localisations
2. Calculer les salaires moyens par r√©gion
3. Cr√©er une carte interactive avec Plotly
4. Ajouter des filtres par technologie et niveau d'exp√©rience

**Fonctionnalit√©s √† ajouter** :
- Comparaison entre 2-3 r√©gions
- Graphiques de distribution des salaires
- Tableaux comparatifs

#### Exercice 3 : Syst√®me de Recommandation

**Objectif** : Recommander des emplois bas√©s sur un profil

**T√¢ches** :
1. Cr√©er un formulaire pour saisir un profil utilisateur :
   - Technologies connues
   - Niveau d'exp√©rience
   - Localisation pr√©f√©r√©e
   - Type de contrat souhait√©
2. Cr√©er un algorithme de scoring
3. Trier les emplois par score de correspondance
4. Afficher les top 10 recommandations

**Algorithme de scoring** :
- +10 points par technologie correspondante
- +20 points si niveau d'exp√©rience correspond
- +15 points si localisation correspond
- +10 points si type de contrat correspond

---

## üêõ R√©solution de Probl√®mes

### Probl√®me : Le scraping ne fonctionne pas

#### Sympt√¥mes
- Erreur de connexion
- Aucune donn√©e extraite
- Erreurs 403/404

#### Solutions

1. **V√©rifier la connexion Internet**
   ```bash
   ping google.com
   ```

2. **V√©rifier que le site est accessible**
   - Ouvrez le site dans votre navigateur
   - V√©rifiez qu'il n'y a pas de maintenance

3. **Inspecter le code HTML**
   - Ouvrez les outils d√©veloppeur (F12)
   - V√©rifiez si la structure HTML a chang√©
   - Adaptez les s√©lecteurs CSS si n√©cessaire

4. **V√©rifier robots.txt**
   ```bash
   curl https://site.com/robots.txt
   ```

5. **Augmenter les d√©lais**
   - Modifiez `REQUEST_DELAY` dans le notebook
   - Essayez avec 3-5 secondes

### Probl√®me : Le dashboard ne charge pas les donn√©es

#### Sympt√¥mes
- Message "Aucune donn√©e trouv√©e"
- Erreur de lecture du CSV
- Dashboard vide

#### Solutions

1. **V√©rifier que le fichier CSV existe**
   ```bash
   ls -la data/donnees_marche_emploi.csv
   ```

2. **Utiliser les donn√©es d'exemple**
   ```bash
   cp examples/donnees_emploi_echantillon.csv data/donnees_marche_emploi.csv
   ```

3. **V√©rifier le format du CSV**
   - Ouvrez le fichier dans un √©diteur de texte
   - V√©rifiez qu'il n'est pas vide
   - V√©rifiez l'encodage (doit √™tre UTF-8)

4. **V√©rifier les colonnes requises**
   Le CSV doit contenir au minimum :
   - `job_title`
   - `company_name`
   - `location`
   - `work_mode`
   - `experience_level`

5. **Consulter les logs**
   - Regardez le terminal o√π Streamlit tourne
   - Cherchez les messages d'erreur

### Probl√®me : Erreurs d'importation

#### Sympt√¥mes
- `ModuleNotFoundError`
- `ImportError`
- Packages non trouv√©s

#### Solutions

1. **V√©rifier l'environnement virtuel**
   ```bash
   which python
   # Doit pointer vers votre venv
   ```

2. **R√©installer les d√©pendances**
   ```bash
   pip install -r requirements.txt --upgrade
   ```

3. **V√©rifier l'installation**
   ```bash
   pip list | grep pandas
   pip list | grep streamlit
   ```

4. **R√©installer un package sp√©cifique**
   ```bash
   pip uninstall pandas
   pip install pandas
   ```

### Probl√®me : Les donn√©es ne s'affichent pas correctement

#### Sympt√¥mes
- Graphiques vides
- Valeurs manquantes
- Format incorrect

#### Solutions

1. **V√©rifier le format des colonnes**
   ```python
   import pandas as pd
   df = pd.read_csv('data/donnees_marche_emploi.csv')
   print(df.dtypes)
   print(df.head())
   ```

2. **V√©rifier les valeurs manquantes**
   ```python
   print(df.isnull().sum())
   ```

3. **Nettoyer les donn√©es**
   - Remplacer les valeurs manquantes
   - Convertir les types de donn√©es
   - Normaliser les formats

4. **V√©rifier les colonnes attendues**
   Assurez-vous que ces colonnes existent :
   - `job_title`, `company_name`, `location`
   - `work_mode`, `experience_level`
   - `salary_min`, `salary_max` (optionnel)
   - `tech_stack_str` (optionnel)

### Probl√®me : Performance lente

#### Sympt√¥mes
- Scraping tr√®s lent
- Dashboard qui met du temps √† charger

#### Solutions

1. **R√©duire le nombre de pages scrap√©es**
   - Modifiez `MAX_JOBS_TO_SCRAPE`
   - Commencez avec 10-20 emplois

2. **Optimiser les requ√™tes**
   - Utilisez `lxml` comme parser (d√©j√† configur√©)
   - R√©duisez la taille des descriptions extraites

3. **Utiliser le cache Streamlit**
   - Le dashboard utilise d√©j√† `@st.cache_data`
   - Rechargez la page pour voir les changements

4. **Optimiser les visualisations**
   - Limitez le nombre de points sur les graphiques
   - Utilisez l'√©chantillonnage pour les grandes datasets

---

## üìö Ressources Compl√©mentaires

### Documentation Officielle

- **[BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)** : Guide complet du parsing HTML
- **[Streamlit Documentation](https://docs.streamlit.io/)** : Documentation compl√®te de Streamlit
- **[Pandas Documentation](https://pandas.pydata.org/docs/)** : Guide de r√©f√©rence pour pandas
- **[Plotly Documentation](https://plotly.com/python/)** : Tous les types de graphiques disponibles
- **[Requests Documentation](https://requests.readthedocs.io/)** : Guide pour les requ√™tes HTTP

### Tutoriels et Guides

- **[Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)** : Bonnes pratiques du scraping
- **[Streamlit Tutorial](https://docs.streamlit.io/get-started/tutorials/create-an-app)** : Tutoriel officiel Streamlit
- **[Pandas Tutorial](https://pandas.pydata.org/docs/getting_started/intro_tutorials/)** : Tutoriels pandas pour d√©butants
- **[Python Web Scraping Guide](https://realpython.com/python-web-scraping-practical-introduction/)** : Guide pratique Real Python

### Ressources P√©dagogiques

- **[Data Visualization with Plotly](https://plotly.com/python/)** : Exemples de visualisations
- **[BeautifulSoup Tutorial](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)** : Guide de d√©marrage rapide
- **[Streamlit Components](https://streamlit.io/components)** : Composants suppl√©mentaires

### Outils Utiles

- **[Chrome DevTools](https://developer.chrome.com/docs/devtools/)** : Pour inspecter le HTML
- **[Postman](https://www.postman.com/)** : Pour tester les API
- **[Regex101](https://regex101.com/)** : Pour tester les expressions r√©guli√®res
- **[JSON Formatter](https://jsonformatter.org/)** : Pour formater le JSON

---

## üí¨ Support

Si vous rencontrez des probl√®mes non couverts dans ce guide :

1. **Consultez les messages d'erreur** : Ils contiennent souvent la solution
2. **V√©rifiez la documentation** : Les liens ci-dessus sont tr√®s utiles
3. **Cherchez sur Stack Overflow** : Beaucoup de probl√®mes ont d√©j√† √©t√© r√©solus
4. **Inspectez le code** : Le code est bien comment√© pour vous aider

---

**Bon apprentissage ! üöÄ**

N'h√©sitez pas √† exp√©rimenter et √† adapter le code √† vos besoins. C'est en pratiquant que l'on apprend le mieux !
